# 组件
a1.sources = r1 r2
a1.channels = c1 c2
a1.sinks = k1 k2

# source1
a1.sources.r1.type = org.apche.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
# batchDurationMillis：表示source向channel批量写入数据的等待时间，与batchSize是或的关系
# 如果批量数据达到batchSize，会将该批数据put到channel；
# 如果在batchDurationMillis时间内，批量数据未达到batchSize，也会将该批数据put到channel。
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop222:9092,hadoop223:9092,hadoop224:9092
a1.sources.r1.kafka.topics=topic_start

# source2
a1.sources.r2.type = org.apche.flume.source.kafka.KafkaSource
a1.sources.r2.batchSize = 5000
a1.sources.r2.batchDurationMillis = 2000
a1.sources.r2.kafka.bootstrap.servers = hadoop222:9092,hadoop223:9092,hadoop224:9092
a1.sources.r2.kafka.topics=topic_event

# channel1
a1.channels.c1.type = file
# file channel保存数据的路径
a1.channels.c1.dataDirs = /opt/module/flume-1.7.0/data/behavior1
# file channel保存sink拉取数据的位置信息，用于take事务
a1.channels.c1.checkpointDir = /opt/module/flume-1.7.0/checkpoint/behavior1
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
# 当出现故障时，重试keep-alive次数后再回滚事务，重试次数
a1.channels.c1.keep-alive = 6

# channel2
a1.channels.c2.type = file
a1.channels.c2.dataDirs = /opt/module/flume-1.7.0/data/behavior2
a1.channels.c2.checkpointDir = /opt/module/flume-1.7.0/checkpoint/behavior2
a1.channels.c2.maxFileSize = 2146435071
a1.channels.c2.capacity = 1000000
a1.channels.c2.keep-alive = 6

# sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = logstart-
# Should the timestamp be rounded down (if true, affects all time based escape sequences except %t)
a1.sinks.k1.hdfs.round = false
a1.sinks.k1.hdfs.roundInterval = 10
a1.sinks.k1.hdfs.roundUnit = second

# sink2
a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = logevent-
a1.sinks.k2.hdfs.round = false
a1.sinks.k2.hdfs.roundInterval = 10
a1.sinks.k2.hdfs.roundUnit = second

# 不要产生大量小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

a1.sinks.k2.hdfs.rollInterval = 10
a1.sinks.k2.hdfs.rollSize = 134217728
a1.sinks.k2.hdfs.rollCount = 0

# 控制输出文件时原生文件
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop

a1.sinks.k2.hdfs.fileType = CompressedStream
a1.sinks.k2.hdfs.codeC = lzop

# 关联
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

a1.sources.r2.channels = c2
a1.sinks.k2.channel = c2